<!DOCTYPE html>
<html class="no-js" lang="en-US">
<head>
  <meta charset="utf-8">
  <title>The 5th International Workshop on Multiscale Multimodal Medical Imaging (MMMI 2024) and the 1st Workshop on Machine Learning for Multimodal/-sensor Healthcare Data (ML4MHD 2024)</title>
  <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
	<meta name="description" content="Homepage of the 5th International Workshop on Multiscale Multimodal Medical Imaging (MMMI 2024) and the 1st Workshop on Machine Learning for Multimodal/-sensor Healthcare Data (ML4MHD 2024)" />
  <link rel="stylesheet" type="text/css" href="./screen.css" media="screen" />
  <link rel="stylesheet" href="./w3.css">
  <link  rel="stylesheet" href="./googlefonts.css">	
</head>
<body>
<div style="width: 60%; float:left">
	<h2><strong>5th International Workshop on Multiscale Multimodal Medical Imaging (MMMI 2024)</strong></h2>
	<h2><strong>The 1st Workshop on Machine Learning for Multimodal/-sensor Healthcare Data (ML4MHD 2024)</strong></h2>
  <h4>In conjunction with <a href="https://www.miccai2024.org/">the 27th International Conference on Medical Image Computing and Computer Assisted Intervention</a>, Marrakech, Morocco</h4>
  <!--<h4>Vancouver Convention Center East Building Level 1, Meeting Room 15</h4>-->
  <a href="https://twitter.com/intent/tweet?button_hashtag=MMMI_ML4MHD2024" class="twitter-hashtag-button" data-size="large" data-text="Join us at MMMI/ML4MHD 2024! https://mmmi2024.github.io/ #medicalimaging #MICCAI #aiforhealth" data-show-count="false">Tweet #MMMI_ML4MHD2024</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div style="width: 40%; float:right">
	<a href="http://www.miccai.org/"><img src="./MICCAI-Logo.jpg" alt="" height="50" /></a>
	<a href="http://english.pku.edu.cn/"><img src="./PKU-Logo.png" alt="" height="50" /></a>
  <a href="https://hkust.edu.hk/"><img src="./HKUST-logo.png" alt="" height="50" /></a>
  <a href="https://viterbischool.usc.edu/"><img src="./USC-Logo.png" alt="" height="50" /></a><br />
	<a href="https://engineering.vanderbilt.edu/"><img src="./vanderbilt-logo.png" alt="" height="50" /></a>
  <a href="https://www.sydney.edu.au/engineering/study/undergraduate-courses/biomedical-engineering.html/"><img src="./sydney-logo.png" alt="" height="50" /></a><br />
 
	<a href="https://www.tum.de/"><img src="./TUM_Logo.png" alt="" height="45" /></a>
	<a href="https://www.fau.eu/"><img src="./FAU_Logo.png" alt="" height="50" /></a>
	<a href="https://www.massgeneral.org/Imaging/"><img src="./MGH-Logo.jpg" alt="" height="45" /></a>
	<a href="https://hms.harvard.edu/"><img src="./HMS-Logo.png" alt="" height="45" /></a>
</div>

<div class="w3-bar w3-black">
  <button class="w3-bar-item w3-button" onclick="openTab('Home')">Home</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Schedule')">Schedule</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Submission')">Submission</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Organization')">Organization</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Dates')">Important Dates</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Registration')">Registration</button>
</div>

<div id="Home" class="w3-container tab">
  <h2>Workshop Updates</h2>
    <p hidden><i>April 17th</i>: Submission portal is now open.</p>
    <p><i>April 16th</i>: Workshop listed in the <a href="https://conferences.miccai.org/2024/en/workshops.asp">MICCAI satelite event program</a>. MMMI/ML4MHD 2024 will be held on October 7th as a half-day event. </p>
  
  <h2>Scope</h2>
  <p>The International Workshop on Multiscale Multimodal Medical Imaging (MMMI) aims at tackling the important challenge of acquiring and analyzing medical images at multiple scales and/or from multiple modalities, which has been increasingly applied in research studies and clinical practice. MMMI offers an opportunity to present: 1) techniques involving multi-modal image acquisition and reconstruction, or imaging at multi-scales; 2) novel methodologies and insights of multiscale multimodal medical images analysis, including image fusing, multimodal augmentation, and joint inference; and 3) empirical studies involving the application of multiscale multimodal imaging for clinical use.</p>
  <p>We are excited to broaden the scope of our typical topics by collaborating with the International Workshop on Multimodal Healthcare Data. This partnership enables us to expand the expertise to multimodal/multisensing healthcare data, with the goal of integrating image knowledge with other modalities.</p>
   <p></p>
  <h2>Objective</h2>
  <p>Facing the growing amount of data available from multiscale multimodal medical imaging facilities and a variety of new methods for the image analysis developed so far, this MICCAI workshop aims to move the forward state of the art in multiscale multimodal medical imaging, including both algorithm development, implementation of the methodology, and experimental studies. The workshop also aims to facilitate more communications and interactions between researchers in the field of medical image analysis and the field of machine learning, especially with expertise in data fusion, multi-fidelity methods, and multi-source learning. In MMMI 2024, weâ€™ll emphasize the potential of artificial general intelligence (AGI) and large-pretrained models in multi-modal, multi-scale medical imaging data. </p>
  <h2>Topics</h2>
  Topic of submissions to the workshop include, but not limited to: <br />
  <li>Image segmentation techniques based on multiscale multimodal images</li>
  <li>Novel techniques in multiscale multimodal image acquisition and reconstruction</li>
  <li>Registration methods across multiscale multimodal images</li>
  <li>Fusion of images from multiple resolutions and novel visualization methods</li>
  <li>Spatial-temporal analysis using multiple modalities</li>
  <li>Fusion of image sources with different fidelities: e.g., co-analysis of EEG and fMRI</li>
  <li>Multiscale multimodal disease diagnosis/prognosis using supervised or unsupervised methods</li>
  <li>Atlas-based methods on multiple imaging modalities</li>
  <li>Cross-modality image generative methods: e.g., generation of synthetic CT/MR images</li>
  <li>Novel radiomics methods based on multiscale multimodal imaging</li>
  <li>Shape analysis on images from multiple sources and/or multiple resolutions</li>
  <li>Graph methods in medical image analysis</li>
  <li>Benchmark studies for multiscale multimodal image analysis: e.g., using electrophysiological signals to validate fMRI data</li>
  <li>Multi-view machine learning for cancer diagnosis and prognosis</li>
  <li>Integrated radiology, pathology, and genomics analysis via learning algorithms</li>
  <li>New image biomarker identification through multiscale multimodal data</li>
  <li>Integrated learning using both image and non-image data</li>
  <li>Multimodal fusion and learning in medical imaging, digital pathology, computational biology, genetics, electronic healthcare records, language/speech processing, and more.</li>
  <li>Multimodal biomarkers for early prediction of disease onset, therapeutic response or disease recurrence</li>
  <li>Benchmarking, domain shifts, and generalization of AI/ML in multimodal healthcare data</li>

  <h2>History of MMMI</h2>
  <p>
  MMMI 2019 (https://mmmi2019.github.io/) recorded 80 attendees and received 18 8-pages submissions, with 13 accepted and presented. The theme of MMMI 2019 was the emerging techniques for imaging and analyzing multi-modal multi-scale data. The 2nd MMMI workshop was merged with MLCDS 2021 (http://mcbr-cds.org/), recorded 58 attendees, and received 16 8-pages submissions, with 10 of them accepted and presented. The theme of MLCDS 2021 was the role and prospect of multi-modal multi-scale imaging in clinical practice. The 3rd MMMI workshop recorded 64 attendees and received 18 8-pages submissions, with 12 of them accepted and presented. The theme of MMMI 2022 was the novel methodology development for multi-modal fusion. The 4th MMMI workshop recorded 70 attendees and received 27 8-page submissions, with 17 accepted and presented. The theme of MMMI 2023 was the solutions for heterogeneities in multi-modal and multi-source medical imaging data. As multi-modal, multi-scale medical imaging is a fast-growing field, we are continuing the MMMI to provide a platform for presenting and discussing novel research from both the radiology and computer science communities. 
  </p>

<h2>History ML4MHD</h2>
	<p>
		Machine Learning for Multimodal Healthcare Data was successfully hosted at the International Conference on Machine Learning 2023 (https://sites.google.com/view/ml4mhd2023), with 33 submissions, 13 published papers and more than 60 attendees. The theme of the workshop was on learning methods for multimodal/multisensing healthcare data such as medical imaging, digital pathology, computational biology, genetics, electronic healthcare records, language/speech processing, and more. 
	</p>
</div>

<div id="Schedule" class="w3-container tab" style="display:none">
  <h2>TBD</h2>
</div>

<div id="Submission" class="w3-container tab" style="display:none">
  <h2>Submission</h2>
  <h5>MMMI/ML4MHD 2024 uses Microsoft CMT for online submission: <a href="https://cmt3.research.microsoft.com/MMMI2024">https://cmt3.research.microsoft.com/MMMI2024</a></h5>
  <h4><strong>Proceedings:</strong></h4>
  <li>Accepted submissions to the MMMI 2024 will be published as Springer <strong>Lecture Notes in Computer Science (LNCS)</strong> series. Selected submissions will also be invited to be included in a workshop-themed special issue published in journals such as IEEE TMI, JBHI,  CMIG, neurocomputing, etc.</li>
  <li>Authors of each accepted paper need to upload a single zip file including the following materials:<br />
    1. A completed copyright form, which can be downloaded <a href="https://mmmi2024.github.io/CopyrightForm_LNCS_MMMI2024.docx">here</a>.<br />
    2. A PDF of the camera-ready version of the submission. Page limitation: the camera-ready manuscript could not exceed 14 pages, with a maximum of 12 pages (text, figures, and tables) + up to 2 pages for references only. The template can be found at <a href="www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines">here</a>.<br />
    3. Source files of the camera-ready, namely, a Word file or a .tex file plus all figures, style files, special fonts, .eps, .bib, etc.<br />
  The zipped file should be named as MMMI_ML4MHD-2024-Paper_ID.zip (e.g., MMMI_ML4MHD-2024-01.zip). In case a large camera-ready package cannot be submitted on the CMT platform, please send the zipped file to Xiang Li (email: xli60@mgh.harvard.edu).</li>
  <h4><strong>Paper Formatting:</strong></h4>
  <li>Papers are limited to <strong>12 pages</strong>, plus <strong>2 pages</strong> of reference. Papers should be formatted in Lecture Notes in Computer Science style. Style files can be found on the <strong>Springer website</strong>. The file format for submissions is Adobe Portable Document Format (PDF). Other formats will not be accepted.</li>
  <li>Authors should consult Springer <a href="https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines" target="_blank">authors guidelines</a> and use their proceedings templates, either for <a href="https://resource-cms.springernature.com/springer-cms/rest/v1/content/19238648/data/v6" target="_blank" rel="noopener">LaTeX</a> or for <a href="https://resource-cms.springernature.com/springer-cms/rest/v1/content/19238706/data/v1" target="_blank" rel="noopener">Word</a>, for the preparation of their papers. Springer encourages authors to include their <a href="https://orcid.org/" target="_blank" rel="noopener">ORCID</a> identifier in their papers. In addition, the corresponding author of each paper, acting on behalf of all of the authors of that paper, must complete and sign a License to Publish form, which can be downloaded <a href="https://mmmi2024.github.io/LicenseToPublishForm_LNCS_MMMI2024.docx">here</a>. The corresponding author signing the copyright form should match the corresponding author marked on the paper. Once the files have been sent to Springer, changes relating to the authorship of the papers cannot be made.</li>
  <h4><strong>Blind Review:</strong></h4>
  <li>Reviewing is <u>strictly double blind</u>: authors do not know the names of the reviewers of their papers, and reviewers do not know the names of the authors. Please see the <strong>anonymity guidelines</strong> of MICCAI 2024 for detailed explanations of how to ensure this.</li>
  <h4><strong>Supplemental Material</strong></h4>
  <li>Supplemental material submission is optional. This material may include: videos of results that cannot be included in the main paper anonymized related submissions to other conferences and journals, and appendices or technical reports containing extended proofs and mathematical derivations that are not essential for understanding of the paper. Contents of the supplemental material should be referred to appropriately in the paper and the reviewers are not obliged to look at it.</li>
  <h4><strong>Simultaneous Submission</strong></h4>
  <li>Our policy is that in submitting a paper, authors implicitly acknowledge that NO paper of substantially similar content has been or will be submitted to another conference or workshop until MMMI decisions are made.</li>
</div>

<div id="Organization" class="w3-container tab" style="display:none">
  <h2>Organization</h2>
  <h3><b>General Chairs</b></h3>
  <ui>
    <img src="./xiang.jpg" height="80" /><strong>Xiang Li</strong><br />
    Assistant Professor, Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA<br />
    Email: xli60@mgh.harvard.edu<br />
    </ui>
	
  <ui>
    <img src="./AndreasM.jpg" height="80" /><strong>Andreas Maier</strong><br />
    Professor, Pattern Recognition Lab, Friedrich-Alexander University Erlangen-Nuremberg, Germany<br />
    Email: andreas.maier@fau.de<br />
    </ui>
  <h3><b>Workshop Chairs</b></h3>

  <img src="./bin.jpg" height="80" /><strong>Bin Dong</strong><br />
  Associate Professor, Beijing International Center for Mathematical Research (BICMR), Peking University, Beijing, China<br />
  Email: dongbin@math.pku.edu.cn<br /><br />
  </ui>
  <ui>
    <img src="./DanielR.jpeg" height="80" /><strong>Daniel Rueckert</strong><br />
    Professor, School of Computation, Information and Technology , Technical University of Munich, Germany<br />
    Email:  daniel.rueckert@tum.de<br />
  </ui>
  <ui>
    <img src="./haochen.jpg" height="80" /><strong>Hao Chen</strong><br />
    Assistant Professor, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong<br />
    Email: jhc@ust.hk<br /><br />
    </ui>  
  <ui>
    <img src="./jinglei.jpg" height="80" /><strong>Jinglei Lv</strong><br />
    Senior Lecturer, School of Biomedical Engineering, University of Sydney, Sydney, Australia<br />
    Email: jinglei.lv@sydney.edu.au<br />
  </ui>
	
  <ui>
    <img src="./PaulaP.jpg" height="80" /><strong>Paula Andrea Perez-Toro</strong><br />
    PhD(c), Pattern Recognition Lab, Friedrich-Alexander University Erlangen-Nuremberg, Germany<br />
    Email: paula.andrea.perez@fau.de<br />
    </ui>
  <ui>
  <img src="./quanzheng.jpg" height="80" /><strong>Quanzheng Li</strong><br />
  Associate Professor, Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA<br />
  Email: li.quanzheng@mgh.harvard.edu<br /><br />
  </ui>
  <ui>
  <img src="./richard.jpg" height="80" /><strong>Richard Leahy</strong><br />
  Deans Professor, Electrical Engineering-Systems, Biomedical Engineering, and Radiology, University of Southern California, Los Angeles, CA<br />
  Email: leahy@sipi.usc.edu<br /><br />
  </ui>  
  <ui>
    <img src="./TomasA.jpg" height="80" /><strong>Tomas Arias-Vergara</strong><br />
    Postdoctoral Researcher, Pattern Recognition Lab, Friedrich-Alexander University Erlangen-Nuremberg, Germany<br />
    Email: tomas.arias@fau.de<br />
    </ui>
  <ui>
    <img src="./xiaoxiao.jpeg" height="80" /><strong>Xiaoxiao Liu</strong><br />
    Assistant Professor, Electrical and Computer Engineering Department , University of British Columbia, Canada<br />
    Email:  xiaoxiao.li@ece.ubc.ca<br />
    </ui>
  <ui>
    <img src="./hui.jpg" height="80" /><strong>Hui Ren</strong><br />
    Instructor in Investigation, Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA<br />
    Email:  hren2@mgh.harvard.edu<br />
    </ui>
  <ui>
    <img src="./yuankai.jpg" height="80" /><strong>Yuankai Huo</strong><br />
    Assistant Professor, School of Engineering, Vanderbilt University, Nashville, TN<br />
    Email: yuankai.huo@vanderbilt.edu<br />
   </ui>
	
  <br />
  <h3 hidden>Program Commitee</h3>
</div>

<div id="Dates" class="w3-container tab" style="display:none">
  <h2>Dates</h2>
  <p>MMMI/ML4MHD 2024 will be held on October 7th PM, 2024 in Marrakech, Morocco<br />
    <li>Deadline for Full Paper Submission: June 24th 2024 23:59, any time on Earth</li>
    <li>Notification of Acceptance: July 8th 2024</li>
    </p>
  </div>

<div id="Registration" class="w3-container tab" style="display:none">
  <h2>Registration</h2>
  <p>Please register to attend the MMMI/ML4MHD 2024 workshop via the MICCAI registration <a href="https://conferences.miccai.org/2024/en/REGISTRATION.html">page</a>.
  </p>
</div>


<script>
function openTab(tabName) {
  var i;
  var x = document.getElementsByClassName("tab");
  for (i = 0; i < x.length; i++) {
    x[i].style.display = "none";  
  }
  document.getElementById(tabName).style.display = "block";  
}
</script>


</body>
</html>
