<!DOCTYPE html>
<html class="no-js" lang="en-US">
<head>
  <meta charset="utf-8">
  <title>5th International Workshop on Multiscale Multimodal Medical Imaging (MMMI 2024)</title>
  <meta http-equiv="Content-Type" content="application/xhtml+xml; charset=utf-8" />
	<meta name="description" content="Homepage of the 4th International Workshop on Multiscale Multimodal Medical Imaging (MMMI 2023)" />
  <link rel="stylesheet" type="text/css" href="./screen.css" media="screen" />
  <link rel="stylesheet" href="./w3.css">
  <link  rel="stylesheet" href="./googlefonts.css">	
</head>
<body>
<div style="width: 60%; float:left">
	<h2><strong>5th International Workshop on </strong></h2>
	<h2><strong>Multiscale Multimodal Medical Imaging (MMMI 2024)</strong></h2>
  <h4>In conjunction with <a href="https://www.miccai2023.org/">26th International Conference on Medical Image Computing and Computer Assisted Intervention</a>, Vancouver, Canada</h4>
  <h4>Vancouver Convention Center East Building Level 1, Meeting Room 15</h4>
  <a href="https://twitter.com/intent/tweet?button_hashtag=MMMI2023" class="twitter-hashtag-button" data-size="large" data-text="Join us at MMMI 2023! https://mmmi2023.github.io/ #medicalimaging #MICCAI #aiforhealth" data-show-count="false">Tweet #MMMI2023</a><script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div style="width: 40%; float:right">
	<a href="http://www.miccai.org/"><img src="./MICCAI-Logo.jpg" alt="" height="50" /></a>
	<a href="http://english.pku.edu.cn/"><img src="./PKU-Logo.png" alt="" height="50" /></a>
  <a href="https://hkust.edu.hk/"><img src="./HKUST-logo.png" alt="" height="50" /></a>
  <a href="https://viterbischool.usc.edu/"><img src="./USC-Logo.png" alt="" height="50" /></a><br />
	<a href="https://engineering.vanderbilt.edu/"><img src="./vanderbilt-logo.png" alt="" height="50" /></a>
  <a href="https://www.sydney.edu.au/engineering/study/undergraduate-courses/biomedical-engineering.html/"><img src="./sydney-logo.png" alt="" height="50" /></a><br />
  <a href="https://www.massgeneral.org/Imaging/"><img src="./MGH-Logo.jpg" alt="" height="45" /></a>
	<a href="https://hms.harvard.edu/"><img src="./HMS-Logo.png" alt="" height="45" /></a>
</div>

<div class="w3-bar w3-black">
  <button class="w3-bar-item w3-button" onclick="openTab('Home')">Home</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Schedule')">Schedule</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Submission')">Submission</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Organization')">Organization</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Dates')">Important Dates</button>
  <button class="w3-bar-item w3-button" onclick="openTab('Registration')">Registration</button>
</div>

<div id="Home" class="w3-container tab">

  <h2>Workshop Updates</h2>
    <p hidden><i>October 13th</i>: Video recordings of the workshop keynote talk and presentations are now available at: <a href="https://youtube.com/playlist?list=PLUEEhy-27RxLokYUHiTdz_4NCyS1CLc_7">YouTube</a>.</p>  
    <p hidden><i>October 13th</i>: Free online access to the workshop proceeding can be downloaded <a href="https://link.springer.com/book/10.1007/978-3-031-18814-5/">here</a>. Note that the full proceeding is only accessible directed from this website, and will be available for 4 weeks.</p>  
    <p><i>October 4th</i>: Full program of the workshop is now online. The workshop will be held at the <strong>Vancouver Convention
      Center East Building Level 1, Meeting Room 15</strong></p>  
    <p><i>August 17th</i>: Notification of acceptance has been sent to the primary authors. Submission of camera-ready version of the accepted submission is now open on CMT-"Create Camera Ready Submission". Deadline for camera-ready submission is <strong>August 22nd 2023</strong>.</p>
    <p hidden><i>July 12th</i>: This year we will have the honor to invite Prof. Arman Rahmim from The University of British Columbia for his keynote speech on "Towards Digital Twins for Precision Medicine"!</p>
    <p><i>August 8th</i>: Due to the large number of submissions we received this year, extra time will be needed for the reviewing process. The notification of acceptance date is delayed to <strong>August 16th 2023</strong>.</p>
    <p><i>July 13th</i>: In response to mulitple requests, submission deadline has been extended to <strong>July 21st 2023</strong>.  </p>
	  <p><i>May 9th</i>: Submission portal is now open.</p>
    <p><i>May 6th</i>: Workshop listed in the <a href="https://conferences.miccai.org/2023/en/MICCAI2023-WORKSHOPS.html">MICCAI satelite event program</a>. MMMI 2023 will be held on October 8th PM as a half-day event. </p>
  <p hidden>We are offering multiple <strong>Best Paper Awards</strong> and <strong>Student Paper Awards</strong>, thanks to the support from our sponsors!<br />
  Because of this, submission deadline has been extended to <strong>August 7th</strong>.  </p>

  <h2>Scope</h2>
  <p>The International Workshop on Multiscale Multimodal Medical Imaging (MMMI) aims at tackling the important challenge of acquiring and analyzing medical images at multiple scales and/or from multiple modalities, which has been increasingly applied in research studies and clinical practice. MMMI offers an opportunity to present: 1) techniques involving multi-modal image acquisition and reconstruction, or imaging at multi-scales; 2) novel methodologies and insights of multiscale multimodal medical images analysis, including image fusing, multimodal augmentation, and joint inference; and 3) empirical studies involving the application of multiscale multimodal imaging for clinical use.</p>
  <p></p>
  <h2>Objective</h2>
  <p>Facing the growing amount of data available from multiscale multimodal medical imaging facilities and a variety of new methods for the image analysis developed so far, this MICCAI workshop aims to move the forward state of the art in multiscale multimodal medical imaging, including both algorithm development, implementation of the methodology, and experimental studies. The workshop also aims to facilitate more communications and interactions between researchers in the field of medical image analysis and the field of machine learning, especially with expertise in data fusion, multi-fidelity methods, and multi-source learning.</p>
  <h2>Topics</h2>
  Topic of submissions to the workshop include, but not limited to: <br />
  <li>Image segmentation techniques based on multiscale multimodal images</li>
  <li>Novel techniques in multiscale multimodal image acquisition and reconstruction</li>
  <li>Registration methods across multiscale multimodal images</li>
  <li>Fusion of images from multiple resolutions and novel visualization methods</li>
  <li>Spatial-temporal analysis using multiple modalities</li>
  <li>Fusion of image sources with different fidelities: e.g., co-analysis of EEG and fMRI</li>
  <li>Multiscale multimodal disease diagnosis/prognosis using supervised or unsupervised methods</li>
  <li>Atlas-based methods on multiple imaging modalities</li>
  <li>Cross-modality image generative methods: e.g., generation of synthetic CT/MR images</li>
  <li>Novel radiomics methods based on multiscale multimodal imaging</li>
  <li>Shape analysis on images from multiple sources and/or multiple resolutions</li>
  <li>Graph methods in medical image analysis</li>
  <li>Benchmark studies for multiscale multimodal image analysis: e.g., using electrophysiological signals to validate fMRI data</li>
  <li>Multi-view machine learning for cancer diagnosis and prognosis</li>
  <li>Integrated radiology, pathology, and genomics analysis via learning algorithms</li>
  <li>New image biomarker identification through multiscale multimodal data</li>
  <li>Integrated learning using both image and non-image data</li>

  <h2>History of MMMI</h2>
  <p>
  MMMI 2019 (https://mmmi2019.github.io/) recorded 80 attendees and received 18 8-pages submissions, with 13 accepted and presented. The theme of MMMI 2019 was the emerging techniques for imaging and analyzing multi-modal multi-scale data. The 2nd MMMI workshop was merged with MLCDS 2021 (http://mcbr-cds.org/), recorded 58 attendees, and received 16 8-pages submissions, with 10 of them accepted and presented. The theme of MLCDS 2021 was the role and prospect of multi-modal multi-scale imaging in clinical practice. The 3rd MMMI workshop recorded 64 attendees and received 18 8-pages submissions, with 12 of them accepted and presented. The theme of MMMI 2022 was the novel methodology development for multi-modal fusion. As multi-modal, multi-scale medical imaging is a fast-growing field; we are continuing the MMMI to provide a platform for presenting and discussing novel research from both the radiology and computer science communities.  
  </p>
</div>

<div id="Schedule" class="w3-container tab" style="display:none">
  <h2>Workshop Schedule</h2>
  <h3>October 8th, 13:30 - 18:00 (Vancouver time)</h3>
  <p>13:30-13:40 <strong>Welcome message and updates from the workshop organization team</strong></br></br>
  13:40-14:30 <strong>Keynote Talk: Prof. Xiaoxiao Li: "Federated Learning on Multi-source and Multi-modal Medical Data"</strong><br />
  Xiaoxiao Li is an Assistant Professor at the Department of Electrical and Computer Engineering at the University of British Columbia (UBC) starting August 2021. In addition, Dr. Li holds positions as a Faculty Member at Vector Institute and an adjunct Assistant Professor at Yale University. Before joining UBC, Dr. Li was a Postdoc Research Fellow at Princeton University. Dr. Li obtained her Ph.D. degree from Yale University in 2020. Dr. Li's research focuses on developing theoretical and practical solutions for enhancing the trustworthiness of AI systems in healthcare. Specifically, her recent research has been dedicated to advancing federated learning techniques and their applications in the medical field. Dr. Li's work has been recognized with numerous publications in top-tier machine learning conferences and journals, including NeurIPS, ICML, ICLR, MICCAI, IPMI, ECCV, TMI, Medical Image Analysis and Nature Methods. Her contributions have been further acknowledged with several best paper awards at prestigious international conferences.<br /><br />
  14:30-15:30 <strong>Long oral session, Part I</strong>
  
  <li>BreastRegNet: A Deep Learning Framework for Registration of Breast Faxitron and Histopathology Images (20m)<br />
  <i><strong>Negar Golestani (Stanford University)</strong> Gregory Bean (Stanford University)  Mirabela Rusu (Stanford University)</i></li><br />

  <li>Identifying Shared Neuroanatomic Architecture between Cognitive Traits through Multiscale Morphometric Correlation Analysis (20m)<br />
  <i><strong>Zixuan Wen (University of Pennsylvania)</strong>  Jingxuan Bao (University of Pennsylvania)  Shannon Risacher (Indiana University)  Andrew Saykin (Indiana University)  Paul Thompson (Imaging Genetics Center )  Christos Davatzikos (University of Pennsylvania)  Yize Zhao (Yale University)  Li Shen (University of Pennsylvania)</i></li><br />

  <li>Modality Cycles with Masked Conditional Diffusion for Unsupervised Anomaly Segmentation in MRI (20m)<br />
  <i><strong>Ziyun Liang (University of Oxford)</strong>  Harry Anthony (University of Oxford)  Felix Wagner (University of Oxford)  Konstantinos Kamnitsas (University of Oxford)</i></li><br />

  15:30-16:00 <strong>Coffee break</strong></br></br>

  16:00-17:00 <strong>Long oral session, Part II</strong><br />

  <li>MAD: Modality Agnostic Distance Measure for Image Registration (20m)<br />
  <i><strong>Vasiliki Sideri-Lampretsa (Technische Universität München)</strong>   Veronika Zimmer (Technical University Munich)  Huaqi Qiu (Imperial College London)  Georgios Kaissis (Technische Universität München)  Daniel Rueckert (Technische Universität München)</i></li><br />

  <li>Osteoarthritis Diagnosis Integrating Whole Joint Radiomics and Clinical Features for Robust Learning Models using Biological Privileged Information (20m) <a href="https://drive.google.com/file/d/18rktmuIlOaVvQW_khKQt5ejMuaqWjNcx/view?usp=sharing">recording</a><br />
  <i><strong>Najla Al Turkestani (King Abdulaziz University)</strong>  Lucia Cevidanes (University of Michigan)  Jonas Bianchi (University of Michigan)  Winston Zhang (University of Michigan)  Marcela Gurgel (University of Michigan)  Baptiste Baquero (University of Michigan)  Reza Soroushmehr (University of Michigan)</i></li><br />  

  <li>Anatomy-Aware Lymph Node Detection in Non-Contrast and Contrast-Enhanced Chest CT using Implicit Station Stratification (20m)<br />
  <i><strong>Ke Yan (Alibaba DAMO Academy)</strong>  Dakai Jin (Alibaba USA Inc.)  Dazhou Guo (Alibaba DAMO Academy USA)  Minfeng Xu (Alibaba)  Na Shen (Zhongshan Hospital of Fudan University)  Xian-Sheng Hua (Damo Academy, Alibaba Group)  Xianghua Ye (Zhejiang University)  Le Lu (Alibaba Group)</i></li><br />

  17:00-18:00 <strong>Short oral session</strong><br />
  <li>M^2Fusion: Bayesian-based Multimodal Multi-level Fusion on Colorectal Cancer Microsatellite Instability Prediction (5m)<br />
  <i>Quan Liu (Vanderbilt University)  Jiawen Yao (DAMO Academy, Alibaba Group)  Lisha Yao (Guangdong)  Xin Chen (Guangzhou First People's Hospital)  Jingren Zhou (Alibaba Group)  Le Lu (Alibaba Group)  Ling Zhang (Alibaba USA Inc.)  Zaiyi Liu (Department of Radiology, Guangdong General Hospital, Guangdong Academy of Medical Science)  Yuankai Huo (Vanderbilt University)</i></li>
  <li>Query Re-Training for Modality-Gnostic Incomplete Multi-modal Brain Tumor Segmentation (5m)<br />
  <i><strong>Delin Chen (Wuhan University)</strong>  YanSheng Qiu (Wuhan University)  Zheng Wang (Wuhan University)</i></li>
  <li>Multimodal Context-Aware Detection of Glioma Biomarkers using MRI and WSI (5m)<br />
  <i><strong>Tomé Albuquerque (INESC TEC)</strong>  Benedikt Wiestler (TUM)  Maria Vasconcelos (Fraunhofer Portugal AICOS)  Jaime Cardoso (INESC Porto, Universidade do Porto)  Peter Schüffler (Technical University of Munich)</i></li>
  <li>Synthesising brain iron maps from quantitative magnetic resonance images using interpretable generative adversarial networks (5m)<br />
  <i><strong>Lindsay Munroe (King's College London)</strong>  Maria Deprez (King's College London)</i></li>
  <li>Noisy-Consistent Pseudo Labeling Model for Semi-supervised Skin Lesion Classification (5m) <a href="https://drive.google.com/file/d/1x99U-xJdnwAf48WolLk0GwR2ZdEYcU5t/view?usp=sharing">recording</a><br />
  <i>Sen Li (Yizhun Medical AI)  <strong>Qian Li (China Aerospace Science and Industry Group 731 Hospital)</strong></i></li>
  <li>Hessian-based Similarity Metric for Multimodal Medical Image Registration (5m)<br />
  <i><strong>Mohammadreza Eskandari (McGill University</strong>) Houssem-Eddine Gueziri (McGill University)  Louis Collins (McGill)</i></li>
  <li>Hybrid Multimodality Fusion with Cross-Domain Knowledge Transfer to Forecast Progression Trajectories in Cognitive Decline (5m)<br />
  <i><strong>Minhui Yu (The University of North Carolina at Chapel Hill)</strong> Yunbi Liu (School of Science and Engineering, The Chinese University of Hong Kong, Shenzhen)  Shijun Qiu (The First Affiliated Hospital of Guangzhou University of Chinese Medicine)  Ling Yue (Department of Geriatric Psychiatry, Shanghai Mental Health Center, Shanghai Jiao Tong University School of Medicine)  Mingxia Liu (University of North Carolina at Chapel Hill</i>)</li>
  <li>Leveraging Contrastive Learning with SimSiam for the Classification of Primary and Secondary Liver Cancers (5m)<br />
  <i><strong>Ramtin Mojtahedi (Queen's University)</strong> Mohammad Hamghalam (Queen's University)  William Jarnagin (Memorial Sloan Kettering Cancer Center)  Richard Do (Memorial Sloan Kettering Cancer Centre)  Amber Simpson (Queen's University)</i></li>
  <li>MuST: Multimodal Spatiotemporal Graph-Transformer for Hospital Readmission Prediction (5m) <a href="https://drive.google.com/file/d/13FIlAPfA3GJFnc8b1TZVFX5aeUlrAmKW/view?usp=sharing">recording</a><br />
  <i><strong>Yan Miao (The University of Hong Kong)</strong> Lequan Yu (The University of Hong Kong)</i></li>
  <li>Groupwise Image Registration with Atlas of Multiple Resolutions Refined at Test Phase (5m)<br />
  <i>Ziyi HE (Hong Kong University of Science and Technology)  Tony C. W. Mok (DAMO Academy, Alibaba Group)  Albert C. S. Chung (HKUST)</i></li>
  </p>
  <p>Presenters are encouraged, although not required, to bring and present the poster associated with the accepted submission. The poster hall is at the Ground Level Exhibition B-C where the coffee break and lunches will be served. There will be labels on the poster board with the acronyms of MMMI. Posters should be in Portrait format. The maximum poster size for MICCAI 2023 is A0, (i.e. 841 x 1189 mm or 33.1 x 46.8 in) (Width x Height) portrait format. </p>
</div>

<div id="Submission" class="w3-container tab" style="display:none">
  <h2>Submission</h2>
  <h5>MMMI uses Microsoft CMT for online submission: <a href="https://cmt3.research.microsoft.com/MMMI2023">https://cmt3.research.microsoft.com/MMMI2023</a></h5>
  <h4><strong>Proceedings:</strong></h4>
  <li>Accepted submissions to the MMMI 2023 will be published as Springer <strong>Lecture Notes in Computer Science (LNCS)</strong> series. Selected submissions will also be invited to be included in a workshop-themed special issue published in journals such as IEEE TMI, JBHI,  CMIG, neurocomputing, etc.</li>
  <li>Authors of each accepted paper need to upload a single zip file including the following materials:<br />
    1. A completed copyright form, which can be downloaded <a href="https://mmmi2023.github.io/CopyrightForm_LNCS_MMMI2023.docx">here</a>.<br />
    2. A PDF of the camera-ready version of the submission. Page limitation: the camera-ready manuscript could not exceed 14 pages, with a maximum of 12 pages (text, figures, and tables) + up to 2 pages for references only. The template can be found at <a href="www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines">here</a>.<br />
    3. Source files of the camera-ready, namely, a Word file or a .tex file plus all figures, style files, special fonts, .eps, .bib, etc.<br />
  The zipped file should be named as MMMI-2023-Paper_ID.zip (e.g., MMMI-2023-01.zip). In case a large camera-ready package cannot be submitted on the CMT platform, please send the zipped file to Xiang Li (email: xli60@mgh.harvard.edu).</li>
  <li hidden>The MMMI 2022 proceedings has been published as Springer <strong>Lecture Notes in Computer Science (LNCS)</strong> series. Link can be found <a href="https://link.springer.com/book/10.1007/978-3-031-18814-5">here</a></li></hide>
  <h4><strong>Paper Formatting:</strong></h4>
  <li>Papers are limited to <strong>12 pages</strong>, plus <strong>2 pages</strong> of reference. Papers should be formatted in Lecture Notes in Computer Science style. Style files can be found on the <strong>Springer website</strong>. The file format for submissions is Adobe Portable Document Format (PDF). Other formats will not be accepted.</li>
  <li>Authors should consult Springer <a href="https://www.springer.com/gp/computer-science/lncs/conference-proceedings-guidelines" target="_blank">authors guidelines</a> and use their proceedings templates, either for <a href="https://resource-cms.springernature.com/springer-cms/rest/v1/content/19238648/data/v6" target="_blank" rel="noopener">LaTeX</a> or for <a href="https://resource-cms.springernature.com/springer-cms/rest/v1/content/19238706/data/v1" target="_blank" rel="noopener">Word</a>, for the preparation of their papers. Springer encourages authors to include their <a href="https://orcid.org/" target="_blank" rel="noopener">ORCID</a> identifier in their papers. In addition, the corresponding author of each paper, acting on behalf of all of the authors of that paper, must complete and sign a Consent-to-Publish form, which can be downloaded <a href="https://mmmi2023.github.io/CopyrightForm_LNCS_MMMI2023.docx">here</a>. The corresponding author signing the copyright form should match the corresponding author marked on the paper. Once the files have been sent to Springer, changes relating to the authorship of the papers cannot be made.</li>
  <h4><strong>Blind Review:</strong></h4>
  <li>Reviewing is <u>strictly double blind</u>: authors do not know the names of the reviewers of their papers, and reviewers do not know the names of the authors. Please see the <strong>anonymity guidelines</strong> of MICCAI 2023 for detailed explanations of how to ensure this.</li>
  <h4><strong>Supplemental Material</strong></h4>
  <li>Supplemental material submission is optional. This material may include: videos of results that cannot be included in the main paper anonymized related submissions to other conferences and journals, and appendices or technical reports containing extended proofs and mathematical derivations that are not essential for understanding of the paper. Contents of the supplemental material should be referred to appropriately in the paper and the reviewers are not obliged to look at it.</li>
  <h4><strong>Simultaneous Submission</strong></h4>
  <li>Our policy is that in submitting a paper, authors implicitly acknowledge that NO paper of substantially similar content has been or will be submitted to another conference or workshop until MMMI decisions are made.</li>
</div>

<div id="Organization" class="w3-container tab" style="display:none">
  <h2>Organization</h2>
  <h3>Workshop Chairs</h3>
  <ui>
  <img src="./quanzheng.jpg" height="80" /><strong>Quanzheng Li</strong><br />
  Associate Professor, Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA<br />
  Email: li.quanzheng@mgh.harvard.edu<br /><br />
  </ui>
  <ui>
  <img src="./richard.jpg" height="80" /><strong>Richard Leahy</strong><br />
  Deans Professor, Electrical Engineering-Systems, Biomedical Engineering, and Radiology, University of Southern California, Los Angeles, CA<br />
  Email: leahy@sipi.usc.edu<br /><br />
  </ui>  
  <img src="./bin.jpg" height="80" /><strong>Bin Dong</strong><br />
  Associate Professor, Beijing International Center for Mathematical Research (BICMR), Peking University, Beijing, China<br />
  Email: dongbin@math.pku.edu.cn<br /><br />
  </ui>
  <ui>
    <img src="./haochen.jpg" height="80" /><strong>Hao Chen</strong><br />
    Assistant Professor, Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong<br />
    Email: jhc@ust.hk<br /><br />
    </ui>  
  <ui>
    <img src="./yuankai.jpg" height="80" /><strong>Yuankai Huo</strong><br />
    Assistant Professor, School of Engineering, Vanderbilt University, Nashville, TN<br />
    Email: yuankai.huo@vanderbilt.edu<br />
    </ui>
  <ui>
    <img src="./jinglei.jpg" height="80" /><strong>Jinglei Lv</strong><br />
    Senior Lecturer, School of Biomedical Engineering, University of Sydney, Sydney, Australia<br />
    Email: jinglei.lv@sydney.edu.au<br />
    </ui>    
  <ui>
    <img src="./xiang.jpg" height="80" /><strong>Xiang Li</strong><br />
    Assistant Professor, Department of Radiology, Massachusetts General Hospital and Harvard Medical School, Boston, MA<br />
    Email: xli60@mgh.harvard.edu<br />
    </ui>
  <br />
  <h3>Program Commitee</h3>
    <ui>Abder-Rahman Ali (aali25@mgh.harvard.edu), Massachusetts General Hospital and Harvard Medical School<br /></ui>	
    <ui>Cheng Chen (cchen101@mgh.harvard.edu), Massachusetts General Hospital and Harvard Medical School<br /></ui>	
    <ui>Ho Hin (hohinlee@microsoft.com), Microsoft<br /></ui>	
    <ui>Hui Ren (hren2@mgh.harvard.edu), Massachusetts General Hospital and Harvard Medical School<br /></ui>	
    <ui>Jiang Hu (jihu@mgh.harvard.edu), Massachusetts General Hospital and Harvard Medical School<br /></ui> 	
    <ui>Lu Zhang (lu.zhang2@mavs.uta.edu), University of Texas at Arlington <br /></ui>	
    <ui>Luyang Luo (cseluyang@ust.hk), The Hong Kong University of Science and Technology<br /></ui> 	
    <ui>Mariano Cabezas (mariano.cabezas@sydney.edu.au), The University of Sydney<br /></ui>	
    <ui>Peng Guo (pguo@mgh.harvard.edu), Massachusetts General Hospital and Harvard Medical School<br /></ui>	
    <ui>Sekeun Kim (skim207@mgh.harvard.edu), Massachusetts General Hospital and Harvard Medical School<br /></ui>	
    <ui>Shijie Zhao (shijiezhao666@gmail.com), Northwestern Polytechnical University<br /></ui>	
    <ui>Shuxing Bao (shunxing.bao@Vanderbilt.Edu), Vanderbilt University<br /></ui> 	
    <ui>Ye Wu (wuye@njust.edu.cn), Nanjing University of Science and Technology<br /></ui>	
    <ui>Yingxue Xu (yxueb@connect.ust.hk), The Hong Kong University of Science and Technology<br /></ui> 	
</div>

<div id="Dates" class="w3-container tab" style="display:none">
  <h2>Dates</h2>
  <p>MMMI 2023 will be held on Oct. 8th PM, 2023, Vancouver, Canada <br />
    <li>Deadline for Full Paper Submission: <strike>July 14th 2023</strike>July 21st 2023 23:59, any time on Earth</li>
    <li>Notification of Acceptance: <strike>July 28th 2023</strike>August 16th 2023</li>
    <li>Deadline for Camera Ready Submission: <strike>August 4th 2023</strike>August 22nd 2023</li>
    </p>
  </div>

<div id="Registration" class="w3-container tab" style="display:none">
  <h2>Registration</h2>
  <p>Please register to attend the MMMI workshop via the MICCAI registration <a href="https://conferences.miccai.org/2023/en/REGISTRATION.html">page</a>.
  </p>
</div>


<script>
function openTab(tabName) {
  var i;
  var x = document.getElementsByClassName("tab");
  for (i = 0; i < x.length; i++) {
    x[i].style.display = "none";  
  }
  document.getElementById(tabName).style.display = "block";  
}
</script>


</body>
</html>
